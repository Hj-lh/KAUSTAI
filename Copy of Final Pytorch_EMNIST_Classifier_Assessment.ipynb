{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1swzpOADUeEhHTsqVn7GGJ2yuZWK7yzK3","timestamp":1700747394132},{"file_id":"1e4jTDp5D43Ey3VtlwdTMDs6FLfCrQMZE","timestamp":1700734067160}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Task: Train an EMNIST classifier\n","\n","EMNIST (or extended MNIST) is just like MNIST, a bunch of handwritten images, except instead of just digits (0-9) it also has uppercase and lowercase characters.\n","\n","You are provided with the code to load the EMNIST train and split **datasets**\n","\n","Write and train a classifier for EMNIST. Make sure to\n","\n","- Show how your loss(es) dropped during training.\n","- Use the testset as a validation set duirng training.\n","- Show final performance of your model on untrained data.\n","- Maintain good code quality.\n"],"metadata":{"id":"zemWtDr8gVOf"}},{"cell_type":"code","source":["from IPython.display import clear_output"],"metadata":{"id":"4GQMlmw2hwbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pip install torch torchvision\n","%pip install matplotlib\n","\n","clear_output()"],"metadata":{"id":"cvcXCUETh0AW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8j3LQ1Qza-vV"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","from torchvision.datasets import EMNIST\n","import torchvision.transforms.functional as F\n","from torchvision.transforms.functional import to_tensor\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n"]},{"cell_type":"code","source":["train_data = EMNIST(root='emnist_data/', split='byclass', download=True,train = True, transform = to_tensor)\n","test_data = EMNIST(root='emnist_data/', split='byclass', download=True, train=False, transform = to_tensor)"],"metadata":{"id":"InCfHKZme7ga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This cell is for demo purposes. feel free to remove it if you want.\n","\n","# print('Number of classes in dataset:', len(train_data.classes))\n","# print('Unique labels:', train_data.classes)\n","\n","# demo_img, demo_label = train_data[10]\n","\n","# print(demo_label)\n","# to_tensor.resize(demo_img, (256, 256)) #resizing just for display"],"metadata":{"id":"udb4rY5Vhaau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","\n","train_loader = DataLoader(train_data, batch_size=batch_size)\n","test_loader = DataLoader(test_data, batch_size=batch_size)\n"],"metadata":{"id":"ySAIszG_Vh5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oH4fRGEGrzN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NN1Layer(nn.Module):\n","  def __init__(self, num_inp, num_out):\n","    super(NN1Layer, self).__init__()\n","    self.layer_1 = nn.Linear(num_inp, num_out)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","\n","    z = self.layer_1(x)\n","    a = self.softmax(z)\n","\n","    return a\n","\n","class NN2Layer(nn.Module):\n","\n","  def __init__(self, num_inp, num_hidden, num_out):\n","\n","    super(NN2Layer, self).__init__()\n","\n","    self.layer_1 = nn.Linear(num_inp, num_hidden)\n","    self.layer_2 = nn.Linear(num_hidden, num_out)\n","\n","    self.hidden_activation = nn.ReLU()  # We can change the hidden activation (activation in between layer 1 and 2) here\n","    self.softmax = nn.Softmax(dim=1)  # dim 0 is normally batch size, we don't want to apply softmax across batch size\n","\n","  def forward(self, x):\n","\n","    z1 = self.layer_1(x)\n","    a1 = self.hidden_activation(z1)\n","\n","    z2 = self.layer_2(a1)\n","    a2 = self.softmax(z2)\n","\n","    return a2"],"metadata":{"id":"oC1BF6FdLLZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","lr = 1e-4\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  # checks if machine supports cuda and if it does, we use that, otherwise cpu\n","\n","train_losses = []\n","val_losses = []\n","\n","# model = NN1Layer(28*28, 10)  # 28*28 because that's the input side. 10 because that's the numbber of classes (0-9)\n","model = NN2Layer(28*28, 32, 10)  # The 2 layer one is equivalent to the one we implemented in numpy\n","\n","optimizer = Adam(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()  # multi-class\n","\n","model.to(device)\n","\n","print(f'Using device {device}')"],"metadata":{"id":"mh07NxlXLp07","executionInfo":{"status":"ok","timestamp":1700747227661,"user_tz":-180,"elapsed":4,"user":{"displayName":"am4 _m03","userId":"03884497344136522816"}},"outputId":"c5323ab7-334b-4897-ebb0-1700e290079b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cpu\n"]}]},{"cell_type":"code","source":["for epoch_no in range(num_epochs):\n","\n","  model.train()  # convert to train model. This turns out train-specific layers in the model (if you dont know about them, an example of them is dropout. more on this later)\n","\n","  epoch_weighted_loss = 0\n","\n","  for batch_X, batch_y in train_loader:\n","\n","    batch_X = batch_X.view(-1, 28*28).to(device)  # convert to [N, 28*28] shape where N is batch_size\n","    batch_y = batch_y.to(device)\n","\n","    batch_y_probs = model(batch_X)  # outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\n","\n","    loss = criterion(batch_y_probs, batch_y)\n","\n","    optimizer.zero_grad()  # need to clear out gradients from previous batch\n","    loss.backward()  # calculate new gradients\n","    optimizer.step()  # update weights\n","\n","    epoch_weighted_loss += (len(batch_y)*loss.item())\n","\n","  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n","  train_losses.append(epoch_loss)    # add loss for tracking. we'll visualize the loss trajectory later\n","\n","\n","  # validation time\n","\n","  model.eval()  # take model to evaluation mode. turn off train-only layers\n","  correctly_labelled = 0\n","\n","  with torch.no_grad():  # this makes our model to NOT track gradients\n","\n","    val_epoch_weighted_loss = 0\n","\n","    for val_batch_X, val_batch_y in test_loader:\n","\n","      val_batch_X = val_batch_X.view(-1, 28*28).to(device)\n","      val_batch_y = val_batch_y.to(device)\n","\n","      val_batch_y_probs = model(val_batch_X)\n","\n","      loss = criterion(val_batch_y_probs, val_batch_y)\n","      val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n","\n","      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n","\n","      correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()  # item converts tensor to float/int/list\n","\n","  val_epoch_loss = val_epoch_weighted_loss/len(test_loader.dataset)\n","  val_losses.append(val_epoch_loss)\n","\n","  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(test_loader.dataset)} correctly ({correctly_labelled/len(test_loader.dataset)*100}% accuracy)')\n","\n","print(f'Training complete on device {device}. Change device variable and run again to see the difference.')"],"metadata":{"id":"2WtyzYRAOy6g","executionInfo":{"status":"error","timestamp":1700747230588,"user_tz":-180,"elapsed":5,"user":{"displayName":"am4 _m03","userId":"03884497344136522816"}},"outputId":"7c8fa76c-b970-4d86-a492-ae322aced97a","colab":{"base_uri":"https://localhost:8080/","height":477}},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-6b2e32f8bcb9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbatch_y_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# need to clear out gradients from previous batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 35 is out of bounds."]}]},{"cell_type":"code","source":["%%time\n","for epoch_no in range(num_epochs):\n","\n","  model.train()  # convert to train model. This turns out train-specific layers in the model (if you dont know about them, an example of them is dropout. more on this later)\n","\n","  epoch_weighted_loss = 0\n","\n","  for batch_X, batch_y in train_loader:\n","\n","    batch_X = batch_X.view(-1, 28*28).to(device)  # convert to [N, 28*28] shape where N is batch_size\n","    batch_y = batch_y.to(device)\n","\n","    batch_y_probs = model(batch_X)  # outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\n","\n","    loss = criterion(batch_y_probs, batch_y)\n","\n","    optimizer.zero_grad()  # need to clear out gradients from previous batch\n","    loss.backward()  # calculate new gradients\n","    optimizer.step()  # update weights\n","\n","    epoch_weighted_loss += (len(batch_y)*loss.item())\n","\n","  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n","  train_losses.append(epoch_loss)    # add loss for tracking. we'll visualize the loss trajectory later\n","\n","\n","  # validation time\n","\n","  model.eval()  # take model to evaluation mode. turn off train-only layers\n","  correctly_labelled = 0\n","\n","  with torch.no_grad():  # this makes our model to NOT track gradients\n","\n","    val_epoch_weighted_loss = 0\n","\n","    for val_batch_X, val_batch_y in test_loader:\n","\n","      val_batch_X = val_batch_X.view(-1, 28*28).to(device)\n","      val_batch_y = val_batch_y.to(device)\n","\n","      val_batch_y_probs = model(val_batch_X)\n","\n","      loss = criterion(val_batch_y_probs, val_batch_y)\n","      val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n","\n","      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n","\n","      correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()  # item converts tensor to float/int/list\n","\n","  val_epoch_loss = val_epoch_weighted_loss/len(test_loader.dataset)\n","  val_losses.append(val_epoch_loss)\n","\n","  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(test_loader.dataset)} correctly ({correctly_labelled/len(test_loader.dataset)*100}% accuracy)')\n","\n","print(f'Training complete on device {device}. Change device variable and run again to see the difference.')"],"metadata":{"id":"siw-aJD-PvUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the Breast Cancer dataset\n","\n","X = train_data.data\n","y = test_data\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","X_test  = torch.tensor(X_test, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","y_test  = torch.tensor(y_test, dtype=torch.float32)"],"metadata":{"id":"dIVcbxlSRxvi","executionInfo":{"status":"error","timestamp":1700747147205,"user_tz":-180,"elapsed":1597,"user":{"displayName":"am4 _m03","userId":"03884497344136522816"}},"outputId":"b3fb6c9b-857d-40b1-9bde-b70034068b43","colab":{"base_uri":"https://localhost:8080/","height":460}},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-bc273e3f44f0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Standardize the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [697932, 116323]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ij3v5AsMaYHX"},"execution_count":null,"outputs":[]}]}